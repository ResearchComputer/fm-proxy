[["Map",1,2,9,10,69,70],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.5.4","content-config-digest","b747f7bb2f0d09ff","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://research.computer\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"svg\":{\"mode\":\"inline\"},\"serializeConfig\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false},\"legacy\":{\"collections\":false}}","articles",["Map",11,12,38,39],"01-deltazip",{"id":11,"data":13,"body":23,"filePath":24,"digest":25,"rendered":26,"legacyId":37},{"title":14,"description":15,"date":16,"demoURL":17,"repoURL":18,"authors":19},"DeltaZip: Multi Full Fine-tuned LLM Serving","Fine-tuning large language models (LLMs) greatly improves model quality for downstream tasks. However, serving many fine-tuned LLMs concurrently is challenging due to the sporadic, bursty, and varying request patterns of different LLMs. To bridge this gap, we present DeltaZip, an LLM serving system that efficiently serves multiple full-parameter fine-tuned models concurrently by aggressively compressing model deltas by up to 10x while maintaining high model quality. The key insight behind this design is that fine-tuning results in small-magnitude changes to the pre-trained model. By co-designing the serving system with the compression algorithm, DeltaZip achieves 2x to 12x improvement in throughput compared to the state-of-the-art systems.",["Date","2024-03-21T23:00:00.000Z"],"https://arxiv.org/abs/2312.05215","https://github.com/eth-easl/deltazip",[20],{"name":21,"url":22},"Xiaozhe Yao","https://about.yao.sh/","[Model Zoo](https://huggingface.co/deltazip).\n\nFine-tuning large language models (LLMs) greatly improves model quality for downstream tasks. However, serving many fine-tuned LLMs concurrently is challenging due to the sporadic, bursty, and varying request patterns of different LLMs. To bridge this gap, we present DeltaZip, an LLM serving system that efficiently serves multiple full-parameter fine-tuned models concurrently by aggressively compressing model deltas by up to 10x while maintaining high model quality. The key insight behind this design is that fine-tuning results in small-magnitude changes to the pre-trained model. By co-designing the serving system with the compression algorithm, DeltaZip achieves 2x to 12x improvement in throughput compared to the state-of-the-art systems.","src/content/articles/01-deltazip/index.md","dd1b4a4285836d6a",{"html":27,"metadata":28},"\u003Cp>\u003Ca href=\"https://huggingface.co/deltazip\">Model Zoo\u003C/a>.\u003C/p>\n\u003Cp>Fine-tuning large language models (LLMs) greatly improves model quality for downstream tasks. However, serving many fine-tuned LLMs concurrently is challenging due to the sporadic, bursty, and varying request patterns of different LLMs. To bridge this gap, we present DeltaZip, an LLM serving system that efficiently serves multiple full-parameter fine-tuned models concurrently by aggressively compressing model deltas by up to 10x while maintaining high model quality. The key insight behind this design is that fine-tuning results in small-magnitude changes to the pre-trained model. By co-designing the serving system with the compression algorithm, DeltaZip achieves 2x to 12x improvement in throughput compared to the state-of-the-art systems.\u003C/p>",{"headings":29,"localImagePaths":30,"remoteImagePaths":31,"frontmatter":32,"imagePaths":36},[],[],[],{"title":14,"description":15,"date":33,"authors":34,"demoURL":17,"repoURL":18},"Mar 22 2024",[35],{"name":21,"url":22},[],"01-deltazip/index.md","02-hexgen",{"id":38,"data":40,"body":42,"filePath":54,"digest":55,"rendered":56,"legacyId":68},{"title":41,"description":42,"date":43,"demoURL":44,"repoURL":45,"authors":46},"HexGen: Generative Inference of Large Language Model over Heterogeneous Environment","Serving generative inference of the large language model is a crucial component of contemporary AI applications. This paper focuses on deploying such services in a heterogeneous and cross-datacenter setting to mitigate the substantial inference costs typically associated with a single centralized datacenter. Towards this end, we propose HexGen, a flexible distributed inference engine that uniquely supports the asymmetric partition of generative inference computations over both tensor model parallelism and pipeline parallelism and allows for effective deployment across diverse GPUs interconnected by a fully heterogeneous network. We further propose a sophisticated scheduling algorithm grounded in constrained optimization that can adaptively assign asymmetric inference computation across the GPUs to fulfill inference requests while maintaining acceptable latency levels. We conduct an extensive evaluation to verify the efficiency of HexGen by serving the state-of-the-art Llama-2 (70B) model. The results suggest that HexGen can choose to achieve up to 2.3 times lower latency deadlines or tolerate up to 4 times more request rates compared with the homogeneous baseline given the same budget.",["Date","2024-03-21T23:00:00.000Z"],"https://arxiv.org/abs/2311.11514","https://github.com/Relaxed-System-Lab/HexGen",[47,50,53],{"name":48,"url":49},"Youhe Jiang","https://youhe-jiang.github.io/",{"name":51,"url":52},"Yan Ran","https://ranyangit.github.io/",{"name":21,"url":22},"src/content/articles/02-hexgen/index.md","a9ca4391e46b0b1f",{"html":57,"metadata":58},"\u003Cp>Serving generative inference of the large language model is a crucial component of contemporary AI applications. This paper focuses on deploying such services in a heterogeneous and cross-datacenter setting to mitigate the substantial inference costs typically associated with a single centralized datacenter. Towards this end, we propose HexGen, a flexible distributed inference engine that uniquely supports the asymmetric partition of generative inference computations over both tensor model parallelism and pipeline parallelism and allows for effective deployment across diverse GPUs interconnected by a fully heterogeneous network. We further propose a sophisticated scheduling algorithm grounded in constrained optimization that can adaptively assign asymmetric inference computation across the GPUs to fulfill inference requests while maintaining acceptable latency levels. We conduct an extensive evaluation to verify the efficiency of HexGen by serving the state-of-the-art Llama-2 (70B) model. The results suggest that HexGen can choose to achieve up to 2.3 times lower latency deadlines or tolerate up to 4 times more request rates compared with the homogeneous baseline given the same budget.\u003C/p>",{"headings":59,"localImagePaths":60,"remoteImagePaths":61,"frontmatter":62,"imagePaths":67},[],[],[],{"title":41,"description":42,"date":33,"authors":63,"demoURL":44,"repoURL":45},[64,65,66],{"name":48,"url":49},{"name":51,"url":52},{"name":21,"url":22},[],"02-hexgen/index.md","guides",["Map",71,72],"01-getting-started",{"id":71,"data":73,"body":76,"filePath":77,"digest":78,"rendered":79,"legacyId":95},{"title":74,"description":74,"date":75},"Getting Started",["Date","2024-03-17T23:00:00.000Z"],"# Getting Started\n\n## Run your Model Locally\n\nWith post-ampere GPU (using [Scratchpad](https://github.com/eth-easl/Scratchpad) as backend):\n\n```bash\ndocker run --rm --gpus all --runtime=nvidia -v $OCF_MODELS:/models -e HF_MODELS=/models -e HF_TOKEN=$HF_TOKEN ghcr.io/researchcomputer/ocf-scratchpad:latest \"sp serve\nQwen/Qwen2.5-7B-Instruct-1M --port 8080 --max-prefill-tokens 8192 --context-length 8192\"\n```\n\nWith pre-ampere GPU (using [Ollama](https://ollama.com/) as backend):\n\n```bash\ndocker run --rm --gpus all --runtime=nvidia -v $OCF_MODELS:/models -e OLLAMA_MODELS=/models ghcr.io/researchcomputer/ocf-ollama:latest gemma3:1b\n```","src/content/guides/01-getting-started/index.md","feac32b0025c069a",{"html":80,"metadata":81},"\u003Ch1 id=\"getting-started\">Getting Started\u003C/h1>\n\u003Ch2 id=\"run-your-model-locally\">Run your Model Locally\u003C/h2>\n\u003Cp>With post-ampere GPU (using \u003Ca href=\"https://github.com/eth-easl/Scratchpad\">Scratchpad\u003C/a> as backend):\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">docker\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> run\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --rm\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --gpus\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> all\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --runtime=nvidia\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -v\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $OCF_MODELS\u003C/span>\u003Cspan style=\"color:#9ECBFF\">:/models\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -e\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> HF_MODELS=/models\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -e\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> HF_TOKEN=\u003C/span>\u003Cspan style=\"color:#E1E4E8\">$HF_TOKEN \u003C/span>\u003Cspan style=\"color:#9ECBFF\">ghcr.io/researchcomputer/ocf-scratchpad:latest\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> \"sp serve\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"color:#9ECBFF\">Qwen/Qwen2.5-7B-Instruct-1M --port 8080 --max-prefill-tokens 8192 --context-length 8192\"\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>With pre-ampere GPU (using \u003Ca href=\"https://ollama.com/\">Ollama\u003C/a> as backend):\u003C/p>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"bash\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#B392F0\">docker\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> run\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --rm\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --gpus\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> all\u003C/span>\u003Cspan style=\"color:#79B8FF\"> --runtime=nvidia\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -v\u003C/span>\u003Cspan style=\"color:#E1E4E8\"> $OCF_MODELS\u003C/span>\u003Cspan style=\"color:#9ECBFF\">:/models\u003C/span>\u003Cspan style=\"color:#79B8FF\"> -e\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> OLLAMA_MODELS=/models\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> ghcr.io/researchcomputer/ocf-ollama:latest\u003C/span>\u003Cspan style=\"color:#9ECBFF\"> gemma3:1b\u003C/span>\u003C/span>\u003C/code>\u003C/pre>",{"headings":82,"localImagePaths":90,"remoteImagePaths":91,"frontmatter":92,"imagePaths":94},[83,86],{"depth":84,"slug":85,"text":74},1,"getting-started",{"depth":87,"slug":88,"text":89},2,"run-your-model-locally","Run your Model Locally",[],[],{"title":74,"description":74,"date":93},"Mar 18 2024",[],"01-getting-started/index.md"]